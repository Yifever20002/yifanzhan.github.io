<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Yifan Zhan</title>

    <meta name="author" content="Yifan Zhan">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Yifan Zhan (詹亦凡)
                </p>
                <p>

    I'm currently a D1 student at the Graduate School of Information Science and Technology (IST), The University of Tokyo, supervised by <a href="https://www.ai.u-tokyo.ac.jp/ja/members/yqzheng">Prof. Yinqiang Zheng</a>.<br>
    My hobbies include playing the violin, watching anime, and gaming.<br>
    E-mail: zhan-yifan AT g.ecc.u-tokyo.ac.jp<br>
    Feel free to reach me!

                </p>
                <p style="text-align:center">
                  <a href="yifeverzhan@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=zOX4or0AAAAJ&hl=zh-CN">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/Yifever20002">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="images/prof_pic.jpg"><img style="width:100%;max-width:100%;object-fit: cover;" alt="profile photo" src="images/prof_pic.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  I work on computer vision and computer graphics, with a particular focus on 3D vision. My research interests primarily involve 3D reconstruction, neural rendering, and human avatars.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


      <tr bgcolor="#f7f7f7">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <img src="images/tomie.gif" width="160" alt="your paper gif">
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <p style="margin:0;">
          <img src="images/tomie_figure.png" alt="ToMiE icon" style="height:1em; vertical-align:-0.15em; margin-right:-3px;">
          ToMiE: Towards Explicit Exoskeleton for the Reconstruction of Complicated 3D Human Avatars
        </p>
        <strong style="color:red;">Yifan Zhan</strong>,
        <a href="https://qtzhu.me/">Qingtian Zhu</a>,
        <a href="https://myniuuu.github.io/">Muyao Niu</a>,
        Mingze Ma</a>,
        Jiancheng Zhao</a>,
        <a href="https://zzh-tech.github.io/">Zhihang Zhong</a>,
        Xiao Sun</a>,
        Yu Qiao</a>,
        <a href="https://www.ai.u-tokyo.ac.jp/ja/members/yqzheng">Yinqiang Zheng</a>
        <br>
        <em>ICCV</em>, 2025
        <br>
        <a href="https://arxiv.org/abs/2410.08082">Paper</a>
        /
        <a href="https://github.com/Yifever20002/ToMiE?tab=readme-ov-file">Code</a>
        <p></p>
        <p>
		ToMiE extends the SMPL skeleton by growing external joints for modeling hand-held objects and loose-fitting clothing with explicit animation.
        </p>
      </td>
    </tr>


      <tr bgcolor="#ffffd0">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <img src="images/gast.png" width="160" alt="your paper gif">
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        Sequential Gaussian Avatars with Hierarchical Motion Context
        <br>
        <a href="https://zezeaaa.github.io/">Wangze Xu</a>,
        <strong style="color:red;">Yifan Zhan</strong>,
        <a href="https://zzh-tech.github.io/">Zhihang Zhong</a>,
        Xiao Sun</a>
        <br>
        <em>ICCV</em>, 2025
        <br>
        <a href="https://arxiv.org/pdf/2411.16768">Paper</a>
        <p></p>
        <p>
		By integrating spatial poses and temporal motion hierarchically, GAST improves the accuracy of non-rigid warping and 3DGS rendering.
        </p>
      </td>
    </tr>


      <tr bgcolor="#f7f7f7">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <img src="images/tree-nerv.png" width="160" alt="your paper gif">
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        Tree-NeRV: A Tree-Structured Neural Representation for Efficient Non-Uniform Video Encoding
        <br>
        Jiancheng Zhao</a>,
        <strong style="color:red;">Yifan Zhan</strong>,
        <a href="https://qtzhu.me/">Qingtian Zhu</a>,
        Mingze Ma</a>,
        <a href="https://myniuuu.github.io/">Muyao Niu</a>,
        Zunian Wan</a>,
        Xiang Ji</a>,
        <a href="https://www.ai.u-tokyo.ac.jp/ja/members/yqzheng">Yinqiang Zheng</a>
        <br>
        <em>ICCV</em>, 2025
        <br>
        <a href="https://arxiv.org/pdf/2504.12899">Paper</a>
        <p></p>
        <p>
		Tree-NeRV introduces a binary tree-structured representation with adaptive temporal sampling for efficient and high-quality video reconstruction.
        </p>
      </td>
    </tr>


      <tr bgcolor="#ffffd0">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <img src="images/anicrafter.png" width="160" alt="your paper gif">
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        AniCrafter: Customizing Realistic Human-Centric Animation via Avatar-Background Conditioning in Video Diffusion Models
        <br>
        <a href="https://myniuuu.github.io/">Muyao Niu</a>,
        Mingdeng Cao</a>,
        <strong style="color:red;">Yifan Zhan</strong>,
        <a href="https://qtzhu.me/">Qingtian Zhu</a>,
        Mingze Ma</a>,
        Jiancheng Zhao</a>,
        Yanhong Zeng</a>,
        <a href="https://zzh-tech.github.io/">Zhihang Zhong</a>,
        Xiao Sun</a>,
        <a href="https://www.ai.u-tokyo.ac.jp/ja/members/yqzheng">Yinqiang Zheng</a>
        <br>
        <em>arxiv</em>, 2025
        <br>
        <a href="https://arxiv.org/pdf/2505.20255">Paper</a>
        /
        <a href="https://github.com/MyNiuuu/AniCrafter">Code</a>
        <p></p>
        <p>
		By reframing animation as a restoration task, AniCrafter brings robust character motion to complex backgrounds and poses.
        </p>
      </td>
    </tr>


      <tr bgcolor="#f7f7f7">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <img src="images/allinone.png" width="160" alt="your paper gif">
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        All-in-One Transferring Image Compression from Human Perception to Multi-Machine Perception
        <br>
        Jiancheng Zhao</a>,
        Xiang Ji</a>,
        Zhuoxiao Li</a>,
        Zunian Wan</a>,
        Weihang Ran</a>,
        Mingze Ma</a>,
        <a href="https://myniuuu.github.io/">Muyao Niu</a>,
        <strong style="color:red;">Yifan Zhan</strong>,
        Cheng-Ching Tseng</a>,
        <a href="https://www.ai.u-tokyo.ac.jp/ja/members/yqzheng">Yinqiang Zheng</a>
        <br>
        <em>arxiv</em>, 2025
        <br>
        <a href="https://arxiv.org/pdf/2504.12997">Paper</a>
        <p></p>
        <p>
		An asymmetric adaptor framework bridges learned image compression and multi-task vision with a single compact model.
        </p>
      </td>
    </tr>
    

      <tr bgcolor="#ffffd0">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <img src="images/r3-avatars.png" width="160" alt="your paper gif">
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        R3-Avatar: Record and Retrieve Temporal Codebook for Reconstructing Photorealistic Human Avatars
        <br>
        <strong style="color:red;">Yifan Zhan</strong>,
        <a href="https://zezeaaa.github.io/">Wangze Xu</a>,
        <a href="https://qtzhu.me/">Qingtian Zhu</a>,
        <a href="https://myniuuu.github.io/">Muyao Niu</a>,
        Mingze Ma</a>,
        Yifei Liu</a>,
        <a href="https://zzh-tech.github.io/">Zhihang Zhong</a>,
        Xiao Sun</a>,
        <a href="https://www.ai.u-tokyo.ac.jp/ja/members/yqzheng">Yinqiang Zheng</a>
        <br>
        <em>arxiv</em>, 2025
        <br>
        <a href="https://arxiv.org/pdf/2503.12751">Paper</a>
        /
        <a href="https://github.com/Yifever20002/R3Avatars">Code</a>
        <p></p>
        <p>
		R3-Avatar introduces a temporal codebook to unify high-fidelity rendering and animatability for human avatars under limited poses and complex clothing.
        </p>
      </td>
    </tr>


      <tr bgcolor="#f7f7f7">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <img src="images/suika.png" width="160" alt="your paper gif">
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        SUICA: Learning Super-high Dimensional Sparse Implicit Neural Representations for Spatial Transcriptomics
        <br>
        <a href="https://qtzhu.me/">Qingtian Zhu</a>,
        Yumin Zheng</a>,
        Yuling Sang</a>,
        <strong style="color:red;">Yifan Zhan</strong>,
        Ziyan Zhu</a>,
        Jun Ding</a>,
        <a href="https://www.ai.u-tokyo.ac.jp/ja/members/yqzheng">Yinqiang Zheng</a>
        <br>
        <em>ICML</em>, 2025
        <br>
        <a href="https://arxiv.org/pdf/2412.01124">Paper</a>
        /
        <a href="https://github.com/Szym29/SUICA">Code</a>
        <p></p>
        <p>
		SUICA models spatial transcriptomics in a continuous and structure-aware manner, enriching both spatial density and gene expression fidelity.
        </p>
      </td>
    </tr>


      <tr bgcolor="#ffffd0">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <img src="images/mask-gaussian.png" width="160" alt="your paper gif">
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        Maskgaussian: Adaptive 3d gaussian representation from probabilistic masks
        <br>
        Yifei Liu,
        <a href="https://zzh-tech.github.io/">Zhihang Zhong</a>,
        <strong style="color:red;">Yifan Zhan</strong>,
        Sheng Xu, 
        Xiao Sun</a>
        <br>
        <em>CVPR</em>, 2025
        <br>
        <a href="https://arxiv.org/pdf/2412.20522">Paper</a>
        /
        <a href="https://github.com/kaikai23/maskgaussian">Code</a>
        <p></p>
        <p>
		Smarter pruning, better rendering -- mask-based rasterization allows gradient flow to pruned Gaussians for dynamic importance learning.
        </p>
      </td>
    </tr>


      <tr bgcolor="#f7f7f7">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <img src="images/rff.png" width="160" alt="your paper gif">
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        Robustifying Fourier Features Embeddings for Implicit Neural Representations
        <br>
        Mingze Ma</a>,
        <a href="https://qtzhu.me/">Qingtian Zhu</a>,
        <strong style="color:red;">Yifan Zhan</strong>,
        Zhengwei Yin</a>,
        <a href="https://dreamzz5.github.io/">Hongjun Wang</a>,
        <a href="https://www.ai.u-tokyo.ac.jp/ja/members/yqzheng">Yinqiang Zheng</a>
        <br>
        <em>arxiv</em>, 2024
        <br>
        <a href="https://arxiv.org/pdf/2502.05482">Paper</a>
        <p></p>
        <p>
		Bias-free MLPs as adaptive frequency filters enable more stable and accurate implicit neural representations across diverse tasks.
        </p>
      </td>
    </tr>

  
      <tr bgcolor="#ffffd0">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <img src="images/baga.png" width="160" alt="your paper gif">
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        Bundle Adjusted Gaussian Avatars Deblurring
        <br>
        <a href="https://myniuuu.github.io/">Muyao Niu</a>,
        <strong style="color:red;">Yifan Zhan</strong>,
        <a href="https://qtzhu.me/">Qingtian Zhu</a>,
        Zhuoxiao Li</a>,
        Wei Wang</a>,
        <a href="https://zzh-tech.github.io/">Zhihang Zhong</a>,
        Xiao Sun</a>,
        <a href="https://www.ai.u-tokyo.ac.jp/ja/members/yqzheng">Yinqiang Zheng</a>
        <br>
        <em>arxiv</em>, 2024
        <br>
        <a href="https://arxiv.org/pdf/2411.16758">Paper</a>
        /
        <a href="https://github.com/MyNiuuu/BAGA">Code</a>
        <p></p>
        <p>
		Joint blur modeling and motion estimation enable high-quality 3D Gaussian avatars from real-world multiview footage.
        </p>
      </td>
    </tr>


      <tr bgcolor="#f7f7f7">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <img src="images/rs-nerf.gif" width="160" alt="your paper gif">
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        RS-NeRF: Neural Radiance Fields from Rolling Shutter Images
        <br>
        <a href="https://myniuuu.github.io/">Muyao Niu</a>,
        Tong Chen</a>,
        <strong style="color:red;">Yifan Zhan</strong>,
        Zhuoxiao Li</a>,
        Xiang Ji</a>,
        <a href="https://www.ai.u-tokyo.ac.jp/ja/members/yqzheng">Yinqiang Zheng</a>
        <br>
        <em>ECCV</em>, 2024
        <br>
        <a href="https://arxiv.org/pdf/2407.10267">Paper</a>
        /
        <a href="https://github.com/MyNiuuu/RS-NeRF">Code</a>
        <p></p>
        <p>
		Rolling shutter-aware modeling enables high-quality novel view synthesis with NeRF from distorted inputs.
        </p>
      </td>
    </tr>


      <tr bgcolor="#ffffd0">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <img src="images/dyco.gif" width="160" alt="your paper gif">
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        Within the Dynamic Context: Inertia-aware 3D Human Modeling with Pose Sequence
        <br>
        Yutong Chen*</a>,
        <strong style="color:red;">Yifan Zhan*</strong>,
        <a href="https://zzh-tech.github.io/">Zhihang Zhong</a>,
        Wei Wang</a>,
        Xiao Sun</a>,
        Yu Qiao</a>,
        <a href="https://www.ai.u-tokyo.ac.jp/ja/members/yqzheng">Yinqiang Zheng</a>
        <br>
        <em>ECCV</em>, 2024
        <br>
        <a href="https://arxiv.org/pdf/2403.19160">Paper</a>
        /
        <a href="https://github.com/Yifever20002/Dyco">Code</a>
        <p></p>
        <p>
		Inertia-aware delta pose modeling enables realistic appearance dynamics for 3D human rendering across similar poses.
        </p>
      </td>
    </tr>



      <tr bgcolor="#f7f7f7">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <img src="images/rpbg.gif" width="160" alt="your paper gif">
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        RPBG: Towards Robust Neural Point-based Graphics in the Wild
        <br>
        <a href="https://qtzhu.me/">Qingtian Zhu</a>,
        Zizhuang Wei</a>,
        Zhongtian Zheng</a>,
        <strong style="color:red;">Yifan Zhan</strong>,
        Zhuyu Yao</a>,
        Jiawang Zhang</a>,
        Kejian Wu</a>,
        <a href="https://www.ai.u-tokyo.ac.jp/ja/members/yqzheng">Yinqiang Zheng</a>
        <br>
        <em>ECCV</em>, 2024 &nbsp <font color=#FF8080><strong>(Oral)</strong></font>
        <br>
        <a href="https://arxiv.org/pdf/2405.05663">Paper</a>
        /
        <a href="https://github.com/QT-Zhu/RPBG">Code</a>
        <p></p>
        <p>
		Point-based representation combined with neural restoration makes high-quality rendering easy and robust.
        </p>
      </td>
    </tr>
    

    <tr bgcolor="#ffffd0">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <img src="images/kfd-nerf.gif" width="160" alt="your paper gif">
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        KFD-NeRF: Rethinking Dynamic NeRF with Kalman Filter
        <br>
        <strong style="color:red;">Yifan Zhan</strong>,
        Zhuoxiao Li</a>,
        <a href="https://myniuuu.github.io/">Muyao Niu</a>,
        <a href="https://zzh-tech.github.io/">Zhihang Zhong</a>,
        <a href="https://shohei.nobuhara.org/index.en.html">Shohei Nobuhara</a>,
        <a href="https://researchmap.jp/ko_nishino">Ko Nishino</a>,
        <a href="https://www.ai.u-tokyo.ac.jp/ja/members/yqzheng">Yinqiang Zheng</a>
        <br>
        <em>ECCV</em>, 2024
        <br>
        <a href="https://arxiv.org/pdf/2407.13185">Paper</a>
        <p></p>
        <p>
		A plug-and-play Kalman-filter-based deformation field that boosts dynamic NeRF rendering quality with minimal network overhead.
        </p>
      </td>
    </tr>


      <tr bgcolor="#f7f7f7">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <img src="images/advnir.PNG" width="160" alt="your paper gif">
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        Physics-Based Adversarial Attack on Near-Infrared Human Detector for Nighttime Surveillance Camera Systems
        <br>
        <a href="https://myniuuu.github.io/">Muyao Niu</a>,
        Zhuoxiao Li</a>,
        <strong style="color:red;">Yifan Zhan</strong>,
        Huy H Nguyen</a>,
        Isao Echizen</a>,
        <a href="https://www.ai.u-tokyo.ac.jp/ja/members/yqzheng">Yinqiang Zheng</a>
        <br>
        <em>ACMMM</em>, 2023
        <br>
        <a href="https://dl.acm.org/doi/pdf/10.1145/3581783.3612082">Paper</a>
        /
        <a href="https://github.com/MyNiuuu/AdvNIR">Code</a>
        <p></p>
        <p>
		An approach that exposes and physically demonstrates attacks on NIR-based surveillance by exploiting material reflectance and sensor sensitivity.
        </p>
      </td>
    </tr>


    <tr bgcolor="#ffffd0">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <img src="images/nerfrac.gif" width="160" alt="your paper gif">
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        Nerfrac: Neural radiance fields through refractive surface
        <br>
        <strong style="color:red;">Yifan Zhan</strong>,
        <a href="https://shohei.nobuhara.org/index.en.html">Shohei Nobuhara</a>,
        <a href="https://researchmap.jp/ko_nishino">Ko Nishino</a>,
        <a href="https://www.ai.u-tokyo.ac.jp/ja/members/yqzheng">Yinqiang Zheng</a>
        <br>
        <em>ICCV</em>, 2023
        <br>
        <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhan_NeRFrac_Neural_Radiance_Fields_through_Refractive_Surface_ICCV_2023_paper.pdf">Paper</a>
        /
        <a href="https://github.com/Yifever20002/NeRFrac">Code</a>
        <p></p>
        <p>
		An approach for novel view synthesis through refractive media by estimating refracted rays with learned refractive fields.
        </p>
      </td>
    </tr>

          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <h2>Awards/Scholarships</h2>
              <p>
                <a href="https://spring-gx.adm.s.u-tokyo.ac.jp/en/boost/">BOOST NAIS Special Research Scholarship</a> (6 students in Japan), The University of Tokyo, <em>2024.10 - 2027.09</em> <br>
                1st Place, ECCV 2024 GigaRendering Challenge, <em>2024.10</em> <br>
                JASSO Scholarship, <em>2022.10 - 2023.04</em> <br>
                Zhiyuan Honors Scholarship (top 5%), Shanghai Jiao Tong University, <em>2018.09 - 2022.06</em> <br>
              </p>
            </td>
          </tr>
        </tbody></table>



<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=300&t=n&d=VpDXPISRwUm2oRAYP0ahu1lLqUnHqh03m40uPpsCXKA'></script>

                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
